{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv(\"https://raw.githubusercontent.com/Evanmj7/Decision-Trees/master/titanic.csv\",\n",
    "                      index_col=0)\n",
    "\n",
    "# Let's ensure the columns we want to treat as continuous are indeed continuous by using pd.to_numeric\n",
    "# The errors = 'coerce' keyword argument will force any values that cannot be\n",
    "# cast into continuous variables to become NaNs.\n",
    "continuous_cols = ['age', 'fare']\n",
    "for col in continuous_cols:\n",
    "    titanic[col] = pd.to_numeric(titanic[col], errors='coerce')\n",
    "\n",
    "# Set categorical cols & convert to dummies\n",
    "cat_cols = ['sex', 'pclass']\n",
    "for col in cat_cols:\n",
    "    titanic[col] = titanic[col].astype('category').cat.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe. An alternative would be to retain some rows with missing values by giving\n",
    "# a special value to nan for each column, eg by imputing some values, but one should be careful not to\n",
    "# use information from the test set to impute values in the training set if doing this. Strictly speaking,\n",
    "# we shouldn't be dropping the nans from the test set here (as we pretend we don't know what's in it) - but\n",
    "# for the sake of simplicity, we will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of regressors\n",
    "\n",
    "\n",
    "# Predicted var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test (25% of data) and train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create an empty decision tree to solve the classification problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last option, ccp_alpha, prunes low-value complexity from the tree to help\n",
    "# avoid overfitting.\n",
    "\n",
    "# Fit the tree with the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the tree:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it perform on the train and test data?\n",
    "train_accuracy = round(clf.score(train[regressors], train[y_var]), 4)\n",
    "print(f'Accuracy on train set is {train_accuracy}')\n",
    "\n",
    "test_accuracy = round(clf.score(test[regressors], test[y_var]), 4)\n",
    "print(f'Accuracy on test set is {test_accuracy}')\n",
    "\n",
    "# Show the confusion matrix\n",
    "plot_confusion_matrix(clf, test[regressors], test[y_var])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2745af96acbee78653c764a6f365ae1c9237a8060f8d2db18cddaa54531309e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
